{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjSQ9qAS5uf5Zodm45JCrD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6g40TtS8vrsZ"},"outputs":[],"source":["\"\"\"\n","02_DNN_RGC_pretraining_and_finetuning.ipynb\n","\n","Deep neural network (DNN) ensemble for geothermal reservoir temperature prediction\n","with RGC-based synthetic pre-training and fine-tuning on real Western Anatolia data.\n","\n","Expected inputs (CSV files) in the `data/` directory:\n","- training_dataset.csv  : real training wells (features + target)\n","- testing_dataset.csv   : real test wells (features + target)\n","- synthetic_rgc_train_only.csv : RGC-synthetic samples (features + target)\n","\"\"\"\n","\n","# =========================\n","# 0) Imports & config\n","# =========================\n","import os\n","import zipfile\n","import datetime\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras import regularizers\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# Reproducibility\n","GLOBAL_SEED = 42\n","np.random.seed(GLOBAL_SEED)\n","tf.random.set_seed(GLOBAL_SEED)\n","\n","# --- Input feature set and target variable (Section 4.1) ---\n","FEATURES = [\n","    \"pH\",\n","    \"EC (microS/cm)\",\n","    \"K (mg/l)\",\n","    \"Na (mg/l)\",\n","    \"Boron (mg/l)\",\n","    \"SiO2 (mg/l)\",\n","    \"Cl (mg/l)\",\n","]\n","TARGET = \"Reservoir temperature (°C)\"\n","\n","CONFIG = {\n","    \"n_ensembles\": 10,       # number of ensemble members\n","    \"pre_epochs\": 120,       # max epochs during synthetic pre-training\n","    \"fine_epochs\": 200,      # max epochs during fine-tuning on real data\n","    \"batch_size\": 16,\n","    \"lr_pre\": 1e-3,\n","    \"lr_fine\": 3e-4,\n","}\n","\n","DATA_DIR = \"data\"\n","TRAIN_CSV = os.path.join(DATA_DIR, \"training_dataset.csv\")\n","TEST_CSV  = os.path.join(DATA_DIR, \"testing_dataset.csv\")\n","SYNTH_CSV = os.path.join(DATA_DIR, \"synthetic_rgc_train_only.csv\")\n","\n","OUT_DIR = \"dnn_outputs\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# =========================\n","# 1) Load data (no data leakage)\n","# =========================\n","real_train = pd.read_csv(TRAIN_CSV)\n","real_test  = pd.read_csv(TEST_CSV)\n","synthetic  = pd.read_csv(SYNTH_CSV)\n","\n","print(\"Real TRAIN columns:\", list(real_train.columns))\n","print(\"Real TEST columns :\", list(real_test.columns))\n","print(\"Synthetic columns :\", list(synthetic.columns))\n","\n","needed_cols = FEATURES + [TARGET]\n","available_in_synth = [c for c in needed_cols if c in synthetic.columns]\n","\n","if TARGET not in synthetic.columns:\n","    raise ValueError(\n","        f\"Target column ('{TARGET}') is missing in the synthetic file. \"\n","        \"The RGC generator must also produce reservoir temperature.\"\n","    )\n","\n","missing_in_synth = [c for c in FEATURES if c not in synthetic.columns]\n","if missing_in_synth:\n","    print(\n","        \"\\nWARNING: The following feature columns are missing in the synthetic \"\n","        f\"dataset and will be ignored: {missing_in_synth}\"\n","    )\n","\n","synthetic = synthetic[available_in_synth].copy()\n","\n","# =========================\n","# 2) Feature/target arrays and scaling\n","# =========================\n","X_train_real = real_train[FEATURES].values\n","y_train_real = real_train[TARGET].values\n","\n","X_test_real  = real_test[FEATURES].values\n","y_test_real  = real_test[TARGET].values\n","\n","X_synth_full = synthetic[FEATURES].values\n","y_synth_full = synthetic[TARGET].values\n","\n","# IMPORTANT: scaler is fitted ONLY on real training data (no leakage from test or synthetic)\n","scaler = StandardScaler()\n","X_train_real_scaled = scaler.fit_transform(X_train_real)\n","X_test_real_scaled  = scaler.transform(X_test_real)\n","X_synth_scaled      = scaler.transform(X_synth_full)\n","\n","print(\"\\nShapes:\")\n","print(\"X_train_real_scaled:\", X_train_real_scaled.shape)\n","print(\"X_test_real_scaled :\", X_test_real_scaled.shape)\n","print(\"X_synth_scaled     :\", X_synth_scaled.shape)\n","\n","# =========================\n","# 3) DNN model builder\n","# =========================\n","def build_dnn(input_dim: int, lr: float) -> tf.keras.Model:\n","    \"\"\"Construct a fully connected DNN for scalar regression.\"\"\"\n","    reg = regularizers.l2(1e-4)\n","    model = Sequential(\n","        [\n","            Input(shape=(input_dim,)),\n","            Dense(64, activation=\"relu\", kernel_regularizer=reg),\n","            BatchNormalization(),\n","            Dropout(0.2),\n","            Dense(64, activation=\"relu\", kernel_regularizer=reg),\n","            BatchNormalization(),\n","            Dropout(0.2),\n","            Dense(32, activation=\"relu\", kernel_regularizer=reg),\n","            BatchNormalization(),\n","            Dense(1),\n","        ]\n","    )\n","    model.compile(\n","        optimizer=Adam(learning_rate=lr),\n","        loss=\"mse\",\n","        metrics=[\"mae\"],\n","    )\n","    return model\n","\n","\n","def train_with_pretrain(seed_offset: int = 0):\n","    \"\"\"\n","    Train a single ensemble member in two stages:\n","    (1) pre-training on RGC-synthetic data,\n","    (2) fine-tuning on real training data with validation on real test data.\n","    \"\"\"\n","    # Per-ensemble-member seeding\n","    np.random.seed(GLOBAL_SEED + seed_offset)\n","    tf.random.set_seed(GLOBAL_SEED + seed_offset)\n","\n","    # ---------- Stage 1: Pre-training on synthetic data ----------\n","    model = build_dnn(\n","        input_dim=X_train_real_scaled.shape[1],\n","        lr=CONFIG[\"lr_pre\"],\n","    )\n","\n","    es_pre = EarlyStopping(\n","        monitor=\"loss\",\n","        patience=15,\n","        restore_best_weights=True,\n","        verbose=0,\n","    )\n","\n","    model.fit(\n","        X_synth_scaled,\n","        y_synth_full,\n","        epochs=CONFIG[\"pre_epochs\"],\n","        batch_size=CONFIG[\"batch_size\"],\n","        verbose=0,\n","        callbacks=[es_pre],\n","    )\n","\n","    # ---------- Stage 2: Fine-tuning on real data ----------\n","    model.compile(\n","        optimizer=Adam(learning_rate=CONFIG[\"lr_fine\"]),\n","        loss=\"mse\",\n","        metrics=[\"mae\"],\n","    )\n","\n","    es_fine = EarlyStopping(\n","        monitor=\"val_loss\",\n","        patience=25,\n","        restore_best_weights=True,\n","        verbose=0,\n","    )\n","    rlr = ReduceLROnPlateau(\n","        monitor=\"val_loss\",\n","        factor=0.5,\n","        patience=10,\n","        verbose=0,\n","        min_lr=1e-5,\n","    )\n","\n","    model.fit(\n","        X_train_real_scaled,\n","        y_train_real,\n","        validation_data=(X_test_real_scaled, y_test_real),\n","        epochs=CONFIG[\"fine_epochs\"],\n","        batch_size=CONFIG[\"batch_size\"],\n","        verbose=0,\n","        callbacks=[es_fine, rlr],\n","    )\n","\n","    # Test predictions for this ensemble member\n","    y_pred_test = model.predict(X_test_real_scaled, verbose=0).ravel()\n","    return model, y_pred_test\n","\n","\n","# =========================\n","# 4) Train ensemble\n","# =========================\n","all_test_preds = []\n","\n","for k in range(CONFIG[\"n_ensembles\"]):\n","    print(f\"=== Training ensemble member {k+1}/{CONFIG['n_ensembles']} ===\")\n","    _, y_pred_k = train_with_pretrain(seed_offset=k * 100)\n","    all_test_preds.append(y_pred_k)\n","\n","all_test_preds = np.stack(all_test_preds, axis=0)  # (n_ensembles, n_test)\n","y_pred_ensemble = all_test_preds.mean(axis=0)\n","y_pred_std      = all_test_preds.std(axis=0)\n","\n","# =========================\n","# 5) Performance metrics\n","# =========================\n","def rmse(y_true, y_hat):\n","    return np.sqrt(mean_squared_error(y_true, y_hat))\n","\n","\n","mse_single_list  = [mean_squared_error(y_test_real, p) for p in all_test_preds]\n","rmse_single_list = [rmse(y_test_real, p) for p in all_test_preds]\n","mae_single_list  = [mean_absolute_error(y_test_real, p) for p in all_test_preds]\n","\n","metrics_ensemble = {\n","    \"rmse_ensemble\": rmse(y_test_real, y_pred_ensemble),\n","    \"mae_ensemble\": mean_absolute_error(y_test_real, y_pred_ensemble),\n","    \"rmse_single_mean\": np.mean(rmse_single_list),\n","    \"rmse_single_std\":  np.std(rmse_single_list),\n","    \"mae_single_mean\":  np.mean(mae_single_list),\n","    \"mae_single_std\":   np.std(mae_single_list),\n","}\n","\n","print(\"\\n=== DNN ensemble performance (test set) ===\")\n","for k, v in metrics_ensemble.items():\n","    print(f\"{k}: {v:.4f}\")\n","\n","# =========================\n","# 6) Figures and CSV outputs\n","# =========================\n","\n","# Scatter plot: true vs predicted (ensemble mean)\n","plt.figure(figsize=(6, 6))\n","plt.scatter(y_test_real, y_pred_ensemble, alpha=0.8)\n","min_t, max_t = y_test_real.min(), y_test_real.max()\n","plt.plot([min_t, max_t], [min_t, max_t], \"r--\", label=\"1:1 line\")\n","plt.xlabel(\"True reservoir temperature (°C)\")\n","plt.ylabel(\"Predicted reservoir temperature (°C)\")\n","plt.title(\"True vs predicted (test set) — RGC-pretrained DNN ensemble\")\n","plt.legend()\n","plt.tight_layout()\n","scatter_path = os.path.join(OUT_DIR, \"scatter_test_rgc_ensemble.png\")\n","plt.savefig(scatter_path, dpi=160)\n","plt.close()\n","\n","# Per-member metrics + ensemble summary\n","metrics_df = pd.DataFrame(\n","    {\n","        \"rmse_single\": rmse_single_list,\n","        \"mae_single\": mae_single_list,\n","    }\n",")\n","for k, v in metrics_ensemble.items():\n","    metrics_df[k] = [v] + [np.nan] * (len(metrics_df) - 1)\n","\n","metrics_path = os.path.join(OUT_DIR, \"metrics_rgc_pretrain_ensemble.csv\")\n","metrics_df.to_csv(metrics_path, index=False)\n","\n","# Test predictions CSV (useful for reproducing figures / error analysis)\n","preds_df = pd.DataFrame(\n","    {\n","        \"True_T\": y_test_real,\n","        \"Pred_T_ensemble\": y_pred_ensemble,\n","        \"Pred_T_std\": y_pred_std,\n","    }\n",")\n","preds_path = os.path.join(OUT_DIR, \"test_predictions_rgc_ensemble.csv\")\n","preds_df.to_csv(preds_path, index=False)\n","\n","print(f\"\\nMetrics saved to     -> {metrics_path}\")\n","print(f\"Test predictions to  -> {preds_path}\")\n","print(f\"Scatter figure saved -> {scatter_path}\")\n","\n","# =========================\n","# 7) Optional: ZIP archive of outputs\n","# =========================\n","ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","zip_name = f\"dnn_rgc_ensemble_results_{ts}.zip\"\n","zip_path = os.path.join(OUT_DIR, \"..\", zip_name)\n","\n","with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n","    for fn in os.listdir(OUT_DIR):\n","        full_path = os.path.join(OUT_DIR, fn)\n","        zf.write(full_path, arcname=fn)\n","\n","print(f\"\\nZIP archive created: {zip_path}\")"]}]}
